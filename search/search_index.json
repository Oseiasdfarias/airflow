{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introdu\u00e7\u00e3o ao Airflow","text":""},{"location":"#o-que-e","title":"O que \u00e9?","text":"<p>Apache Airflow \u00e9 uma plataforma de c\u00f3digo aberto usada para programar, monitorar e gerenciar workflows. Ele permite que os usu\u00e1rios definam pipelines de dados complexos como um c\u00f3digo em Python, facilitando a cria\u00e7\u00e3o, manuten\u00e7\u00e3o e monitoramento de fluxos de trabalho, desde a extra\u00e7\u00e3o de dados at\u00e9 a transforma\u00e7\u00e3o e carregamento (ETL). Airflow \u00e9 altamente escal\u00e1vel, suportando desde pequenos projetos at\u00e9 arquiteturas de dados empresariais. Ele fornece uma interface web intuitiva para visualizar a execu\u00e7\u00e3o dos workflows, al\u00e9m de recursos avan\u00e7ados como agendamento de tarefas, depend\u00eancias din\u00e2micas, execu\u00e7\u00e3o paralela e recupera\u00e7\u00e3o de falhas, tornando-o uma ferramenta robusta e flex\u00edvel para gerenciar pipelines de dados em um ambiente de produ\u00e7\u00e3o.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/","title":"1.1 Usando airflow no Docker","text":""},{"location":"Configurando%20Ambiente/configuracao_ambiente/#como-instalar-o-apache-airflow-no-docker","title":"Como Instalar o Apache Airflow no Docker","text":"<p>Apache Airflow \u00e9 uma plataforma de c\u00f3digo aberto que permite a cria\u00e7\u00e3o, programa\u00e7\u00e3o e monitoramento de workflows program\u00e1veis. Uma maneira eficiente de utilizar o Airflow \u00e9 atrav\u00e9s de cont\u00eaineres Docker. Este artigo fornecer\u00e1 um guia passo a passo para instalar e configurar o Apache Airflow usando Docker.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#indice","title":"\u00cdndice","text":"<ul> <li>Como Instalar o Apache Airflow no Docker</li> <li>\u00cdndice</li> <li>Pr\u00e9-requisitos</li> <li>Passo 1: Instalar Docker<ul> <li>Verifique a instala\u00e7\u00e3o do Docker</li> <li>Verifique a instala\u00e7\u00e3o do Docker Compose</li> </ul> </li> <li>Passo 2: Configurar o Docker Compose</li> <li>Passo 3: Iniciar o Apache Airflow<ul> <li>Inicialize o banco de dados</li> <li>Iniciar os servi\u00e7os do Airflow</li> </ul> </li> <li>Passo 4: Acessar a Interface Web do Airflow</li> <li>Conclus\u00e3o</li> </ul>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Docker instalado no seu sistema. Se voc\u00ea ainda n\u00e3o tem o Docker instalado, siga as instru\u00e7\u00f5es do guia de instala\u00e7\u00e3o do Docker.</li> <li>Docker Compose instalado. As instru\u00e7\u00f5es para instala\u00e7\u00e3o do Docker Compose podem ser encontradas aqui.</li> </ul>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#passo-1-instalar-docker","title":"Passo 1: Instalar Docker","text":"<p>Antes de come\u00e7armos, certifique-se de que o Docker e o Docker Compose est\u00e3o instalados no seu sistema.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#verifique-a-instalacao-do-docker","title":"Verifique a instala\u00e7\u00e3o do Docker","text":"<pre><code>docker --version\n</code></pre>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#verifique-a-instalacao-do-docker-compose","title":"Verifique a instala\u00e7\u00e3o do Docker Compose","text":"<pre><code>docker-compose --version\n</code></pre> <p>Se ambos os comandos retornarem vers\u00f5es instaladas, voc\u00ea est\u00e1 pronto para continuar.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#passo-2-configurar-o-docker-compose","title":"Passo 2: Configurar o Docker Compose","text":"<p>Crie um diret\u00f3rio para o projeto Airflow e navegue at\u00e9 ele:</p> <pre><code>mkdir airflow-docker\ncd airflow-docker\n</code></pre> <p>Dentro desse diret\u00f3rio, crie um arquivo <code>docker-compose.yml</code> com o seguinte conte\u00fado:</p> <pre><code># Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n#\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\n#\n# This configuration supports basic configuration using environment variables or an .env file\n# The following variables are supported:\n#\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n#                                Default: apache/airflow:2.9.1\n# AIRFLOW_UID                  - User ID in Airflow containers\n#                                Default: 50000\n# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n#                                Default: .\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n#\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n#                                Default: airflow\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n#                                Default: airflow\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n#                                Use this option ONLY for quick checks. Installing requirements at container\n#                                startup is done EVERY TIME the service is started.\n#                                A better way is to build a custom image or extend the official image\n#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n#                                Default: ''\n#\n# Feel free to modify this file to suit your needs.\n---\nx-airflow-common:\n  &amp;airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}\n  # build: .\n  environment:\n    &amp;airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'\n    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n    # The following line can be used to set a custom config file, stored in the local config folder\n    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file\n    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &amp;airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n\n  redis:\n    # Redis is limited to 7.2-bookworm due to licencing change\n    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/\n    image: redis:7.2-bookworm\n    expose:\n      - 6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    restart: always\n\n  airflow-webserver:\n    &lt;&lt;: *airflow-common\n    command: webserver\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    &lt;&lt;: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    &lt;&lt;: *airflow-common\n    command: celery worker\n    healthcheck:\n      # yamllint disable rule:line-length\n      test:\n        - \"CMD-SHELL\"\n        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    environment:\n      &lt;&lt;: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: \"0\"\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    &lt;&lt;: *airflow-common\n    command: triggerer\n    healthcheck:\n      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    &lt;&lt;: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available &lt; 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available &lt; 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available &lt; one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        mkdir -p /sources/logs /sources/dags /sources/plugins\n        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n        exec /entrypoint airflow version\n    # yamllint enable rule:line-length\n    environment:\n      &lt;&lt;: *airflow-common-env\n      _AIRFLOW_DB_MIGRATE: 'true'\n      _AIRFLOW_WWW_USER_CREATE: 'true'\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: ''\n    user: \"0:0\"\n    volumes:\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\n\n  airflow-cli:\n    &lt;&lt;: *airflow-common\n    profiles:\n      - debug\n    environment:\n      &lt;&lt;: *airflow-common-env\n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n\n  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    &lt;&lt;: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - \"5555:5555\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\nvolumes:\n  postgres-db-volume:\n</code></pre> <p>Este arquivo define os servi\u00e7os necess\u00e1rios para o Apache Airflow: PostgreSQL como banco de dados, o webserver e o scheduler do Airflow.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#passo-3-iniciar-o-apache-airflow","title":"Passo 3: Iniciar o Apache Airflow","text":""},{"location":"Configurando%20Ambiente/configuracao_ambiente/#inicialize-o-banco-de-dados","title":"Inicialize o banco de dados","text":"<p>Antes de iniciar os servi\u00e7os, voc\u00ea precisa inicializar o banco de dados do Airflow:</p> <pre><code>docker-compose up airflow-init\n</code></pre> <p>Este comando criar\u00e1 as tabelas necess\u00e1rias no banco de dados PostgreSQL. Depois de completar a inicializa\u00e7\u00e3o, voc\u00ea pode parar o servi\u00e7o pressionando <code>Ctrl+C</code>.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#iniciar-os-servicos-do-airflow","title":"Iniciar os servi\u00e7os do Airflow","text":"<p>Inicie todos os servi\u00e7os definidos no <code>docker-compose.yml</code>:</p> <pre><code>docker-compose up\n</code></pre> <p>Isso iniciar\u00e1 os cont\u00eaineres do PostgreSQL, webserver e scheduler.</p>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#passo-4-acessar-a-interface-web-do-airflow","title":"Passo 4: Acessar a Interface Web do Airflow","text":"<p>Uma vez que os servi\u00e7os estejam em execu\u00e7\u00e3o, voc\u00ea pode acessar a interface web do Airflow navegando at\u00e9 <code>http://localhost:8080</code> em seu navegador.</p> <p>Voc\u00ea deve ver a interface de login do Airflow. Use as credenciais padr\u00e3o:</p> <ul> <li>Username: <code>airflow</code></li> <li>Password: <code>airflow</code></li> </ul>"},{"location":"Configurando%20Ambiente/configuracao_ambiente/#conclusao","title":"Conclus\u00e3o","text":"<p>Seguindo estes passos, voc\u00ea deve conseguir instalar e configurar o Apache Airflow em um ambiente Docker. Esta configura\u00e7\u00e3o b\u00e1sica pode ser expandida e customizada conforme necess\u00e1rio, adicionando DAGs, configurando diferentes executores e ajustando par\u00e2metros de configura\u00e7\u00e3o de acordo com suas necessidades.</p> <p>Dockeriza seu ambiente de Airflow n\u00e3o s\u00f3 simplifica a gest\u00e3o dos servi\u00e7os, mas tamb\u00e9m oferece uma forma escal\u00e1vel e port\u00e1til de executar workflows complexos.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/","title":"Guia de Instala\u00e7\u00e3o do Docker em Linux, Windows e Mac","text":"<p>Docker \u00e9 uma plataforma de software que permite criar, testar e implantar aplicativos rapidamente. O Docker empacota o software em unidades padronizadas chamadas cont\u00eaineres que t\u00eam tudo o que o software precisa para funcionar, incluindo bibliotecas, ferramentas do sistema, c\u00f3digo e runtime. A seguir, apresentamos um guia detalhado para a instala\u00e7\u00e3o do Docker nas principais plataformas: Linux, Windows e Mac.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#indice","title":"\u00cdndice","text":"<ol> <li>Instala\u00e7\u00e3o no Linux</li> <li>Instala\u00e7\u00e3o no Windows</li> <li>Instala\u00e7\u00e3o no Mac</li> </ol>"},{"location":"Configurando%20Ambiente/instalar_docker/#instalacao-no-linux","title":"Instala\u00e7\u00e3o no Linux","text":""},{"location":"Configurando%20Ambiente/instalar_docker/#passo-1-atualize-seu-sistema","title":"Passo 1: Atualize seu sistema","text":"<p>Primeiro, certifique-se de que seu sistema est\u00e1 atualizado: <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-2-instale-as-dependencias","title":"Passo 2: Instale as depend\u00eancias","text":"<p>Instale as depend\u00eancias necess\u00e1rias: <pre><code>sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-3-adicione-a-chave-gpg-do-docker","title":"Passo 3: Adicione a chave GPG do Docker","text":"<p>Adicione a chave GPG oficial do Docker: <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-4-adicione-o-repositorio-do-docker","title":"Passo 4: Adicione o reposit\u00f3rio do Docker","text":"<p>Adicione o reposit\u00f3rio do Docker \u00e0s suas fontes APT: <pre><code>echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-5-instale-o-docker","title":"Passo 5: Instale o Docker","text":"<p>Atualize as fontes APT e instale o Docker: <pre><code>sudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-6-verifique-a-instalacao","title":"Passo 6: Verifique a instala\u00e7\u00e3o","text":"<p>Verifique se a instala\u00e7\u00e3o foi bem-sucedida: <pre><code>sudo docker --version\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#instalacao-no-windows","title":"Instala\u00e7\u00e3o no Windows","text":""},{"location":"Configurando%20Ambiente/instalar_docker/#passo-1-baixe-o-docker-desktop","title":"Passo 1: Baixe o Docker Desktop","text":"<p>Baixe o Docker Desktop para Windows no site oficial do Docker.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-2-execute-o-instalador","title":"Passo 2: Execute o instalador","text":"<p>Execute o instalador baixado e siga as instru\u00e7\u00f5es na tela.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-3-inicie-o-docker-desktop","title":"Passo 3: Inicie o Docker Desktop","text":"<p>Ap\u00f3s a instala\u00e7\u00e3o, inicie o Docker Desktop a partir do menu Iniciar.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-4-verifique-a-instalacao","title":"Passo 4: Verifique a instala\u00e7\u00e3o","text":"<p>Abra um terminal do PowerShell e execute: <pre><code>docker --version\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#instalacao-no-mac","title":"Instala\u00e7\u00e3o no Mac","text":""},{"location":"Configurando%20Ambiente/instalar_docker/#passo-1-baixe-o-docker-desktop_1","title":"Passo 1: Baixe o Docker Desktop","text":"<p>Baixe o Docker Desktop para Mac no site oficial do Docker.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-2-instale-o-docker-desktop","title":"Passo 2: Instale o Docker Desktop","text":"<p>Abra o arquivo <code>.dmg</code> baixado e arraste o \u00edcone do Docker para a pasta Aplicativos.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-3-inicie-o-docker-desktop_1","title":"Passo 3: Inicie o Docker Desktop","text":"<p>Abra o Docker Desktop a partir da pasta Aplicativos e siga as instru\u00e7\u00f5es para concluir a instala\u00e7\u00e3o.</p>"},{"location":"Configurando%20Ambiente/instalar_docker/#passo-4-verifique-a-instalacao_1","title":"Passo 4: Verifique a instala\u00e7\u00e3o","text":"<p>Abra o Terminal e execute: <pre><code>docker --version\n</code></pre></p>"},{"location":"Configurando%20Ambiente/instalar_docker/#conclusao","title":"Conclus\u00e3o","text":"<p>Seguindo estes passos, voc\u00ea deve conseguir instalar o Docker em m\u00e1quinas com Linux, Windows e Mac. O Docker \u00e9 uma ferramenta poderosa para o desenvolvimento e implanta\u00e7\u00e3o de aplicativos, e a sua instala\u00e7\u00e3o \u00e9 o primeiro passo para aproveitar todos os benef\u00edcios que ele oferece.</p>"},{"location":"Introdu%C3%A7%C3%A3o%20ao%20Airflow/Vis%C3%A3o%20do%20webserver/","title":"Em desenvolvimento ...","text":""}]}